{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dateset: https://www.statmt.org/europarl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import zipfile\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'pt-en.zip'\n",
    "# zip_object = zipfile.ZipFile(file = path, mode = 'r')\n",
    "# zip_object.extractall('./')\n",
    "# zip_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pt-en/europarl-v7.pt-en.en', mode='r', encoding='utf-8') as f:\n",
    "  europarl_en = f.read()\n",
    "with open('pt-en/europarl-v7.pt-en.pt', mode='r', encoding='utf-8') as f:\n",
    "  europarl_pt = f.read () "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "europarl_en[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = europarl_en.split('\\n')\n",
    "len(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = europarl_pt.split('\\n')\n",
    "len(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_en = europarl_en\n",
    "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
    "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
    "corpus_en = re.sub(r\" +\", \" \", corpus_en)\n",
    "corpus_en = corpus_en.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_pt = europarl_pt\n",
    "corpus_pt = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_pt)\n",
    "corpus_pt = re.sub(r\".\\$\\$\\$\", '', corpus_pt)\n",
    "corpus_pt = re.sub(r\" +\", \" \", corpus_pt)\n",
    "corpus_pt = corpus_pt.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corpus_en),len(corpus_pt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_en, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_pt, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_pt.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_en = tokenizer_en.vocab_size + 2\n",
    "vocab_size_pt = tokenizer_pt.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[vocab_size_en - 2] + tokenizer_en.encode(sentence) + [vocab_size_en - 1] for sentence in corpus_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [[vocab_size_pt - 2] + tokenizer_pt.encode(sentence) + [vocab_size_pt - 1] for sentence in corpus_pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing large sentences \n",
    "#### -For better results, skip this step-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 15\n",
    "idx_to_remove = [count for count, sent in enumerate(inputs) if len(sent) > max_length]\n",
    "len(idx_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in reversed(idx_to_remove):\n",
    "  del inputs[idx]\n",
    "  del outputs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_remove = [count for count, sent in enumerate(outputs) if len(sent) > max_length]\n",
    "len(idx_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in reversed(idx_to_remove):\n",
    "  del inputs[idx]\n",
    "  del outputs[idx]\n",
    "\n",
    "print(len(inputs),len(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding and Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, value=0, padding = 'post', maxlen=max_length)\n",
    "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs, value=0, padding = 'post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "  print(outputs[random.randint(0, len(outputs) - 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "      super(PositionalEncoding, self).__init__()       \n",
    "    \n",
    "    def get_angles(self, pos, i, d_model):\n",
    "      angles = 1 / np.power(10000., (2*(i // 2)) / np.float32(d_model))\n",
    "      return pos * angles # (seq_lenght, d_model)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "      seq_lenght = inputs.shape.as_list()[-2]\n",
    "      d_model = inputs.shape.as_list()[-1]\n",
    "      angles = self.get_angles(np.arange(seq_lenght)[:, np.newaxis],\n",
    "                               np.arange(d_model)[np.newaxis, :], d_model)\n",
    "      angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "      angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "      pos_encoding = angles[np.newaxis, ...]\n",
    "      return inputs + tf.cast(pos_encoding, tf.float32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "  product = tf.matmul(queries, keys, transpose_b=True)\n",
    "  keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "  scaled_product = product / tf.math.sqrt(keys_dim)\n",
    "\n",
    "  if mask is not None:\n",
    "    scaled_product += (mask * -1e9) # 0.0000000001\n",
    "\n",
    "  attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
    "  return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multihead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    \n",
    "    def __init__(self, nb_proj):\n",
    "      super(MultiHeadAttention, self).__init__()\n",
    "      self.nb_proj = nb_proj\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "      self.d_model = input_shape[-1]\n",
    "      assert self.d_model % self.nb_proj == 0\n",
    "\n",
    "      self.d_proj = self.d_model // self.nb_proj\n",
    "\n",
    "      self.query_lin = layers.Dense(units = self.d_model)\n",
    "      self.key_lin = layers.Dense(units = self.d_model)\n",
    "      self.value_lin = layers.Dense(units = self.d_model)\n",
    "\n",
    "      self.final_lin = layers.Dense(units = self.d_model)\n",
    "    \n",
    "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_lenght, d_model)\n",
    "      shape = (batch_size, -1, self.nb_proj, self.d_proj)\n",
    "      splited_inputs = tf.reshape(inputs, shape = shape) # (batch_size, seq_lenght, nb_proj, d_proj)\n",
    "      return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_lenght, d_proj)\n",
    "   \n",
    "    def call(self, queries, keys, values, mask):\n",
    "      batch_size = tf.shape(queries)[0]\n",
    "\n",
    "      queries = self.query_lin(queries)\n",
    "      keys = self.key_lin(keys)\n",
    "      values = self.value_lin(values)\n",
    "\n",
    "      queries = self.split_proj(queries, batch_size)\n",
    "      keys = self.split_proj(keys, batch_size)\n",
    "      values = self.split_proj(values, batch_size)\n",
    "\n",
    "      attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "\n",
    "      attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "      concat_attention = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\n",
    "\n",
    "      outputs = self.final_lin(concat_attention)\n",
    "      \n",
    "      return outputs    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "      super(EncoderLayer, self).__init__()\n",
    "      self.FFN_units = FFN_units\n",
    "      self.nb_proj = nb_proj\n",
    "      self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "      self.d_model = input_shape[-1]\n",
    "\n",
    "      self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
    "      self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "      self.norm_1 = layers.LayerNormalization(epsilon=1e-6) # 0.0000001\n",
    "\n",
    "      self.dense_1 = layers.Dense(units=self.FFN_units, activation='relu')\n",
    "      self.dense_2 = layers.Dense(units=self.d_model, activation='relu')\n",
    "      self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "\n",
    "      self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, mask, training):\n",
    "      attention = self.multi_head_attention(inputs, inputs, inputs, mask)\n",
    "      attention = self.dropout_1(attention, training = training)\n",
    "      attention = self.norm_1(attention + inputs)\n",
    "\n",
    "      outputs = self.dense_1(attention)\n",
    "      outputs = self.dense_2(outputs)\n",
    "      outputs = self.dropout_2(outputs, training=training)\n",
    "      outputs = self.norm_2(outputs + attention)\n",
    "\n",
    "      return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"encoder\"):\n",
    "      super(Encoder, self).__init__(name=name)\n",
    "      self.nb_layers = nb_layers\n",
    "      self.d_model = d_model\n",
    "\n",
    "      self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "      self.pos_encoding = PositionalEncoding()\n",
    "      self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "      self.enc_layers = [EncoderLayer(FFN_units, nb_proj, dropout_rate) for _ in range(nb_layers)]\n",
    "\n",
    "    \n",
    "    def call(self, inputs, mask, training):\n",
    "      outputs = self.embedding(inputs)\n",
    "      outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "      outputs = self.pos_encoding(outputs)\n",
    "      outputs = self.dropout(outputs, training)\n",
    "\n",
    "      for i in range(self.nb_layers):\n",
    "        outputs = self.enc_layers[i](outputs, mask, training)\n",
    "\n",
    "      return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "      super(DecoderLayer, self).__init__()\n",
    "      self.FFN_units = FFN_units\n",
    "      self.nb_proj = nb_proj\n",
    "      self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "      self.d_model = input_shape[-1]\n",
    "\n",
    "      self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
    "      self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "      self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "      self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
    "      self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "      self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "      self.dense_1 = layers.Dense(units = self.FFN_units, activation='relu')\n",
    "      self.dense_2 = layers.Dense(units = self.d_model, activation='relu')\n",
    "      self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
    "      self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "               \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "      attention = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)\n",
    "      attention = self.dropout_1(attention, training)\n",
    "      attention = self.norm_1(attention + inputs)\n",
    "\n",
    "      attention_2 = self.multi_head_attention_2(attention, enc_outputs, enc_outputs, mask_2)\n",
    "      attention_2 = self.dropout_2(attention_2, training)\n",
    "      attention_2 = self.norm_2(attention_2 + attention)\n",
    "\n",
    "      outputs = self.dense_1(attention_2)\n",
    "      outputs = self.dense_2(outputs)\n",
    "      outputs = self.dropout_3(outputs, training)\n",
    "      outputs = self.norm_3(outputs + attention_2)\n",
    "\n",
    "      return outputs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"decoder\"):\n",
    "      super(Decoder, self).__init__(name=name)\n",
    "      self.d_model = d_model\n",
    "      self.nb_layers = nb_layers\n",
    "\n",
    "      self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "      self.pos_encoding = PositionalEncoding()\n",
    "      self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "\n",
    "      self.dec_layers = [DecoderLayer(FFN_units, nb_proj, dropout_rate) for i in range(nb_layers)]\n",
    "        \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "      outputs = self.embedding(inputs)\n",
    "      outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "      outputs = self.pos_encoding(outputs)\n",
    "      outputs = self.dropout(outputs, training)\n",
    "\n",
    "      for i in range(self.nb_layers):\n",
    "        outputs = self.dec_layers[i](outputs, enc_outputs, mask_1, mask_2, training)\n",
    "\n",
    "      return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size_enc,\n",
    "                 vocab_size_dec,\n",
    "                 d_model,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 name=\"transformer\"):\n",
    "        super(Transformer, self).__init__(name=name)\n",
    "\n",
    "        self.encoder = Encoder(nb_layers, FFN_units, nb_proj, dropout_rate, \n",
    "                               vocab_size_enc, d_model)\n",
    "        self.decoder = Decoder(nb_layers, FFN_units, nb_proj, dropout_rate,\n",
    "                               vocab_size_dec, d_model)\n",
    "        self.last_linear = layers.Dense(units=vocab_size_dec, name='lin_output')\n",
    "                \n",
    "    def create_padding_mask(self, seq): # (batch_size, seq_length) -> (batch_size, nb_proj, seq_lenght, d_proj)\n",
    "      mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "      return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "    \n",
    "    def create_look_ahead_mask(self, seq):\n",
    "      seq_len = tf.shape(seq)[1]\n",
    "      look_ahed_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "      return look_ahed_mask\n",
    "    \n",
    "    def call(self, enc_inputs, dec_inputs, training):\n",
    "      enc_mask = self.create_padding_mask(enc_inputs)\n",
    "      dec_mask_1 = tf.maximum(self.create_padding_mask(dec_inputs), self.create_look_ahead_mask(dec_inputs))\n",
    "      dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
    "\n",
    "      enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
    "      dec_outputs = self.decoder(dec_inputs, enc_outputs, dec_mask_1, dec_mask_2, training)\n",
    "\n",
    "      outputs = self.last_linear(dec_outputs)\n",
    "\n",
    "      return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq): # (batch_size, seq_length) -> (batch_size, nb_proj, seq_lenght, d_proj)\n",
    "  mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "    \n",
    "def create_look_ahead_mask(seq):\n",
    "  seq_len = tf.shape(seq)[1]\n",
    "  look_ahed_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  return look_ahed_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = tf.cast([[837, 836, 0, 273, 8, 0, 0, 0]], tf.int32)\n",
    "tf.maximum(create_padding_mask(seq), create_look_ahead_mask(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_padding_mask(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_look_ahead_mask(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "d_model = 128 # 512\n",
    "nb_layers = 4 # 6\n",
    "ffn_units = 512 # 2048\n",
    "nb_proj = 8 # 8\n",
    "dropout_rate = 0.1 # 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(vocab_size_enc=vocab_size_en,\n",
    "                          vocab_size_dec=vocab_size_pt,\n",
    "                          d_model=d_model,\n",
    "                          nb_layers=nb_layers,\n",
    "                          FFN_units=ffn_units,\n",
    "                          nb_proj=nb_proj,\n",
    "                          dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, \n",
    "                                                            reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(target, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "  loss_ = loss_object(target, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = tf.cast(d_model, tf.float32)\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/translater\"\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print('Latest checkpoint restored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "  print('Start or epoch {}'.format(epoch + 1))\n",
    "  start = time.time()\n",
    "\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "\n",
    "  for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
    "    dec_inputs = targets[:, :-1]\n",
    "    dec_outputs_real = targets[:, 1:]\n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions = transformer(enc_inputs, dec_inputs, True)\n",
    "      loss = loss_function(dec_outputs_real, predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(dec_outputs_real, predictions)\n",
    "\n",
    "    if batch % 50 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
    "  \n",
    "  ckpt_save_path = ckpt_manager.save()\n",
    "  print('Saving checkpoint for epoch {} at {}'.format(epoch + 1, ckpt_save_path))\n",
    "  print('Time taken for 1 epoch {} secs\\n'.format(time.time() - start))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'you are smart'\n",
    "text = [vocab_size_en - 2] + tokenizer_en.encode(text) + [vocab_size_en - 1]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tf.expand_dims(text, axis=0)\n",
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.expand_dims([vocab_size_pt - 2], axis = 0)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "  inp_sentence = [vocab_size_en - 2] + tokenizer_en.encode(inp_sentence) + [vocab_size_en - 1]\n",
    "  enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
    "\n",
    "  output = tf.expand_dims([vocab_size_pt - 2], axis = 0)\n",
    "\n",
    "  # i am -> am happy\n",
    "\n",
    "  for _ in range(max_length):\n",
    "    # (1, seq_length, vocab_size)\n",
    "    predictions = transformer(enc_input, output, False)\n",
    "    prediction = predictions[:, -1:, :]\n",
    "\n",
    "    predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "\n",
    "    if predicted_id == vocab_size_pt - 1:\n",
    "      return tf.squeeze(output, axis=0)\n",
    "\n",
    "    output = tf.concat([output, predicted_id], axis=1)\n",
    "  \n",
    "  return tf.squeeze(output, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  output = evaluate(sentence).numpy()\n",
    "\n",
    "  predicted_sentence = tokenizer_pt.decode([i for i in output if i < vocab_size_pt - 2])\n",
    "  \n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Predicted translation: {}'.format(predicted_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"this is a really powerful tool\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
